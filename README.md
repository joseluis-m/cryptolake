# ğŸ”ï¸ CryptoLake â€” Real-Time Crypto Analytics Lakehouse

[![CI Pipeline](https://github.com/tu-usuario/cryptolake/actions/workflows/ci.yml/badge.svg)](https://github.com/tu-usuario/cryptolake/actions/workflows/ci.yml)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg?logo=python&logoColor=white)](https://python.org)
[![Apache Spark](https://img.shields.io/badge/Spark-3.5-E25A1C?logo=apachespark&logoColor=white)](https://spark.apache.org)
[![Apache Iceberg](https://img.shields.io/badge/Iceberg-1.5-4A90D9?logo=apache&logoColor=white)](https://iceberg.apache.org)
[![dbt](https://img.shields.io/badge/dbt-1.8-FF694B?logo=dbt&logoColor=white)](https://getdbt.com)
[![Airflow](https://img.shields.io/badge/Airflow-2.9-017CEE?logo=apacheairflow&logoColor=white)](https://airflow.apache.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

> An end-to-end data engineering platform that ingests real-time and historical
> cryptocurrency data, processes it through a **Medallion Architecture**
> (Bronze â†’ Silver â†’ Gold) on **Apache Iceberg**, transforms with **dbt**,
> orchestrates with **Airflow**, validates with automated quality checks,
> and serves analytics via **REST API** and **interactive dashboard** â€”
> all containerized with Docker and provisioned with Terraform.

---

## ğŸ—ï¸ Architecture

```mermaid
graph TB
    subgraph Sources["ğŸ“¡ Data Sources"]
        BN[Binance WebSocket<br/>Real-time prices]
        CG[CoinGecko API<br/>Historical data]
        FG[Alternative.me<br/>Fear & Greed Index]
    end

    subgraph Ingestion["ğŸ”„ Ingestion"]
        KF[Apache Kafka<br/>Streaming]
        PY[Python Extractors<br/>Batch]
    end

    subgraph Lakehouse["ğŸ”ï¸ Lakehouse â€” MinIO + Apache Iceberg"]
        direction LR
        BR["ğŸ¥‰ Bronze<br/>Raw data<br/>Append-only"]
        SL["ğŸ¥ˆ Silver<br/>Cleaned & deduped<br/>MERGE INTO"]
        GL["ğŸ¥‡ Gold<br/>Star Schema<br/>dbt models"]
        BR -->|Spark Batch| SL
        SL -->|dbt| GL
    end

    subgraph Orchestration["â° Orchestration"]
        AF[Apache Airflow<br/>Daily DAG 06:00 UTC]
    end

    subgraph Quality["âœ… Data Quality"]
        DQ[Custom Validators<br/>15+ checks per run]
    end

    subgraph Serving["ğŸ–¥ï¸ Serving Layer"]
        API[FastAPI<br/>REST API + Swagger]
        ST[Streamlit<br/>Dashboard]
    end

    BN --> KF --> BR
    CG --> PY --> BR
    FG --> PY
    AF --> Ingestion
    AF --> Lakehouse
    AF --> DQ
    GL --> API --> ST
```

## ğŸ› ï¸ Tech Stack

| Layer | Technology | Version | Purpose |
|-------|-----------|---------|---------|
| **Streaming** | Apache Kafka | 3.7 | Real-time price ingestion from Binance |
| **Processing** | Apache Spark | 3.5 | Batch + stream processing (PySpark) |
| **Table Format** | Apache Iceberg | 1.5 | ACID transactions, time travel, schema evolution |
| **Storage** | MinIO | Latest | S3-compatible object storage |
| **Transformation** | dbt-core + dbt-spark | 1.8 | SQL-based dimensional modeling (Kimball) |
| **Orchestration** | Apache Airflow | 2.9 | Pipeline scheduling and monitoring |
| **Data Quality** | Custom Framework | â€” | 15+ automated validation checks |
| **API** | FastAPI | 0.110 | REST API with auto-generated docs |
| **Dashboard** | Streamlit | 1.35+ | Interactive visualizations (Plotly) |
| **Infrastructure** | Docker Compose | 24+ | Local containerized environment |
| **IaC** | Terraform | 1.8+ | AWS S3 provisioning |
| **CI/CD** | GitHub Actions | â€” | Automated testing and deployment |
| **Code Quality** | Ruff | 0.3+ | Linting + formatting |

## ğŸš€ Quick Start

### Prerequisites

- Docker Desktop (6+ CPU cores, 8+ GB RAM)
- Python 3.11+
- Make

### Setup

```bash
# Clone the repository
git clone https://github.com/tu-usuario/cryptolake.git
cd cryptolake

# Configure environment
cp .env.example .env

# Start all services (12+ containers)
make up

# Run the full pipeline
make pipeline
```

### Access Points

| Service | URL |
|---------|-----|
| **API Docs (Swagger)** | http://localhost:8000/docs |
| **Dashboard** | http://localhost:8501 |
| **Airflow UI** | http://localhost:8083 (admin/admin) |
| **MinIO Console** | http://localhost:9001 (cryptolake/cryptolake123) |
| **Spark UI** | http://localhost:8082 |
| **Kafka UI** | http://localhost:8080 |

## ğŸ“Š Data Model

### Medallion Architecture

```
Bronze (Raw)              Silver (Cleaned)           Gold (Business-Ready)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
historical_prices         daily_prices               fact_market_daily
  coin_id                   coin_id                    coin_id (FK)
  timestamp_ms    â”€â”€â–º       price_date       â”€â”€â–º       price_date (FK)
  price_usd                 price_usd                  price_usd
  market_cap_usd            market_cap_usd             moving_avg_7d/30d
  volume_24h_usd            volume_24h_usd             volatility_7d
  _ingested_at              _processed_at              fear_greed_value
  _source                                              ma30_signal
  _loaded_at                                           combined_signal

fear_greed                fear_greed                 dim_coins
  value                     index_date                 coin_id (PK)
  classification            fear_greed_value           all_time_high
  timestamp                 classification             avg_price
  _ingested_at              _processed_at              total_days_tracked
  _source
  _loaded_at                                         dim_dates
                                                       date_day (PK)
                                                       year, month, quarter
                                                       is_weekend
```

### Star Schema (Gold Layer)

```
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    dim_dates      â”‚
              â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚
              â”‚  date_day (PK)   â”‚â—„â”€â”€â”
              â”‚  year, month     â”‚   â”‚
              â”‚  quarter         â”‚   â”‚
              â”‚  is_weekend      â”‚   â”‚
              â”‚  day_name        â”‚   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                                     â”‚  price_date = date_day
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    dim_coins      â”‚   â”‚       fact_market_daily               â”‚
â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚   â”‚       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚
â”‚  coin_id (PK)    â”‚â—„â”€â”€â”¤  coin_id (FK)                         â”‚
â”‚  first_tracked   â”‚   â”‚  price_date (FK)                      â”‚
â”‚  all_time_high   â”‚   â”‚  price_usd                            â”‚
â”‚  avg_price       â”‚   â”‚  market_cap_usd, volume_24h_usd       â”‚
â”‚  avg_daily_vol   â”‚   â”‚  price_change_pct_1d                  â”‚
â”‚  price_range_pct â”‚   â”‚  moving_avg_7d, moving_avg_30d        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  volatility_7d, avg_volume_7d         â”‚
                        â”‚  fear_greed_value, market_sentiment    â”‚
                        â”‚  ma30_signal, combined_signal          â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ˆ Key Features

- **Medallion Architecture** â€” Bronze â†’ Silver â†’ Gold on Apache Iceberg
- **Dual Pipeline** â€” Real-time streaming (Kafka) + daily batch (CoinGecko API)
- **Dimensional Modeling** â€” Kimball star schema with facts and dimensions
- **Incremental Processing** â€” MERGE INTO for efficient upserts in Silver
- **Automated Quality** â€” 15+ data quality checks across all layers
- **REST API** â€” FastAPI with Swagger docs, serving Gold layer analytics
- **Interactive Dashboard** â€” Streamlit with Plotly charts
- **Full Orchestration** â€” Airflow DAG: Ingest â†’ Bronze â†’ Silver â†’ Gold â†’ Quality
- **CI/CD** â€” GitHub Actions: lint, test, dbt compile, Docker build
- **Infrastructure as Code** â€” Terraform modules for AWS S3 provisioning

## ğŸ—‚ï¸ Project Structure

```
cryptolake/
â”œâ”€â”€ .github/workflows/          # CI/CD pipelines
â”‚   â”œâ”€â”€ ci.yml                  # Lint â†’ Test â†’ dbt â†’ Docker
â”‚   â””â”€â”€ data-quality.yml        # Manual quality validation
â”œâ”€â”€ docker/                     # Dockerfiles
â”‚   â”œâ”€â”€ spark/                  # Spark + Iceberg JARs
â”‚   â”œâ”€â”€ airflow/                # Airflow + dbt virtualenv
â”‚   â””â”€â”€ api/                    # FastAPI
â”œâ”€â”€ terraform/                  # Infrastructure as Code
â”‚   â”œâ”€â”€ modules/storage/        # S3 bucket definitions
â”‚   â””â”€â”€ environments/           # Local + AWS configs
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/                 # Pydantic settings
â”‚   â”œâ”€â”€ ingestion/              # Kafka producer + API extractors
â”‚   â”œâ”€â”€ processing/batch/       # Spark jobs (Bronzeâ†’Silverâ†’Gold)
â”‚   â”œâ”€â”€ transformation/         # dbt project (star schema)
â”‚   â”œâ”€â”€ quality/                # Data quality validators
â”‚   â”œâ”€â”€ orchestration/dags/     # Airflow DAG
â”‚   â””â”€â”€ serving/                # FastAPI + Streamlit
â”œâ”€â”€ tests/                      # Pytest unit tests
â”œâ”€â”€ docs/                       # Architecture, data dictionary, contracts
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â””â”€â”€ README.md
```

## ğŸ§ª Testing & Quality

```bash
# Linting
ruff check src/ tests/

# Unit tests
pytest tests/unit/ -v

# dbt model validation
cd src/transformation/dbt_cryptolake && dbt compile --target ci

# Data quality checks (requires services running)
make quality-check

# Full pipeline
make pipeline
```

## ğŸ“– Documentation

- [Architecture Decision Records](docs/architecture.md) â€” Why Iceberg, dbt, Airflow, etc.
- [Data Dictionary](docs/data_dictionary.md) â€” Every field in every table documented
- [Data Contracts](docs/data_contracts/) â€” Schema agreements between pipeline stages
- [Setup Guide](docs/setup_guide.md) â€” Step-by-step from zero to running

## ğŸ“ What I Learned

Building CryptoLake was an exercise in integrating production-grade tools
into a cohesive data platform. Key takeaways:

1. **Iceberg is the future** â€” ACID transactions, time travel, and schema
   evolution on object storage eliminate the need for traditional data
   warehouses for many use cases.

2. **dbt + Spark is powerful but tricky** â€” The Thrift Server bridge works
   well, but requires careful isolation of Python environments to avoid
   dependency conflicts (protobuf versions between Airflow and dbt).

3. **Docker Compose has limits** â€” Running 12+ services locally requires
   careful resource management. Learned to optimize images, health checks,
   and startup ordering.

4. **Data quality is not optional** â€” Automated validation caught several
   issues with API data (null prices, duplicate records, stale timestamps)
   that would have silently corrupted downstream analytics.

5. **The Medallion pattern scales** â€” Separating raw ingestion (Bronze),
   cleaning (Silver), and business modeling (Gold) makes each layer
   independently testable and debuggable.

## ğŸ“„ License

This project is licensed under the MIT License â€” see [LICENSE](LICENSE).

---

*Built as a Data Engineering portfolio project demonstrating production-grade
practices with modern data stack technologies.*

