from __future__ import annotations

"""
Spark Batch Job: Silver â†’ Gold (Modelo Dimensional)

Crea un star schema con:
- dim_coins: DimensiÃ³n con estadÃ­sticas de cada criptomoneda
- dim_dates: DimensiÃ³n calendario con atributos Ãºtiles para anÃ¡lisis
- fact_market_daily: Tabla de hechos con mÃ©tricas diarias y seÃ±ales tÃ©cnicas

Las mÃ©tricas calculadas incluyen:
- Price change % (day-over-day)
- Moving averages (7d, 30d)
- Volatilidad (desviaciÃ³n estÃ¡ndar 7d)
- SeÃ±al MA30 (precio por encima/debajo de media 30d)
- Sentimiento de mercado (Fear & Greed)

EjecuciÃ³n:
    docker exec cryptolake-spark-master \
        /opt/spark/bin/spark-submit /opt/spark/work/src/processing/batch/silver_to_gold.py
"""
from pyspark.sql import SparkSession


def build_dim_coins(spark: SparkSession):
    """
    Construye la dimensiÃ³n dim_coins.

    Tipo: SCD Type 1 (Slowly Changing Dimension Type 1)
    Esto significa que cuando los datos cambian, simplemente sobrescribimos.
    No guardamos historial de cambios en la dimensiÃ³n.

    Â¿CuÃ¡ndo usarÃ­as Type 2? Cuando necesitas saber el valor histÃ³rico.
    Por ejemplo, si un coin cambia de nombre, querrÃ­as saber cÃ³mo se
    llamaba cuando hiciste cierto anÃ¡lisis. Para nuestro caso, Type 1
    es suficiente porque los stats se recalculan cada dÃ­a.
    """
    print("\nğŸ“ Construyendo dim_coins...")

    spark.sql("""
        CREATE OR REPLACE TABLE cryptolake.gold.dim_coins
        USING iceberg
        LOCATION 's3://cryptolake-gold/dim_coins'
        AS
        SELECT
            coin_id,
            
            -- Tracking
            MIN(price_date)                     AS first_tracked_date,
            MAX(price_date)                     AS last_tracked_date,
            COUNT(DISTINCT price_date)          AS total_days_tracked,
            
            -- Price stats
            ROUND(MIN(price_usd), 6)            AS all_time_low,
            ROUND(MAX(price_usd), 2)            AS all_time_high,
            ROUND(AVG(price_usd), 6)            AS avg_price,
            
            -- Volume stats
            ROUND(AVG(volume_24h_usd), 2)       AS avg_daily_volume,
            ROUND(MAX(volume_24h_usd), 2)       AS max_daily_volume,
            
            -- Market cap (Ãºltimo valor conocido)
            ROUND(MAX(market_cap_usd), 2)       AS max_market_cap,
            
            -- Rango de precio (volatilidad histÃ³rica simplificada)
            ROUND(
                ((MAX(price_usd) - MIN(price_usd)) / MIN(price_usd)) * 100,
                2
            )                                   AS price_range_pct,
            
            current_timestamp()                 AS _loaded_at
            
        FROM cryptolake.silver.daily_prices
        GROUP BY coin_id
    """)

    count = spark.table("cryptolake.gold.dim_coins").count()
    print(f"  âœ… dim_coins: {count} coins")


def build_dim_dates(spark: SparkSession):
    """
    Construye la dimensiÃ³n dim_dates (calendario).

    Â¿Por quÃ© una tabla de fechas?
    Porque "2024-02-25" es solo un dato. Pero para anÃ¡lisis necesitas
    saber: Â¿es fin de semana? Â¿quÃ© trimestre? Â¿quÃ© mes?

    En producciÃ³n, esta tabla se carga una vez y cubre varios aÃ±os.
    AquÃ­ la generamos dinÃ¡micamente desde las fechas que tenemos.
    """
    print("\nğŸ“… Construyendo dim_dates...")

    spark.sql("""
        CREATE OR REPLACE TABLE cryptolake.gold.dim_dates
        USING iceberg
        LOCATION 's3://cryptolake-gold/dim_dates'
        AS
        SELECT DISTINCT
            price_date                              AS date_day,
            YEAR(price_date)                        AS year,
            MONTH(price_date)                       AS month,
            DAYOFMONTH(price_date)                  AS day_of_month,
            DAYOFWEEK(price_date)                   AS day_of_week,
            WEEKOFYEAR(price_date)                  AS week_of_year,
            QUARTER(price_date)                     AS quarter,
            
            -- Flags booleanos para anÃ¡lisis
            CASE
                WHEN DAYOFWEEK(price_date) IN (1, 7)
                THEN true ELSE false
            END                                     AS is_weekend,
            
            -- Nombres legibles
            DATE_FORMAT(price_date, 'EEEE')         AS day_name,
            DATE_FORMAT(price_date, 'MMMM')         AS month_name
            
        FROM cryptolake.silver.daily_prices
        ORDER BY date_day
    """)

    count = spark.table("cryptolake.gold.dim_dates").count()
    print(f"  âœ… dim_dates: {count} fechas")


def build_fact_market_daily(spark: SparkSession):
    """
    Construye la tabla de hechos fact_market_daily.

    Esta es la tabla mÃ¡s importante y compleja del star schema.
    Cada fila = 1 criptomoneda Ã— 1 dÃ­a, con todas las mÃ©tricas.

    Window Functions utilizadas:

    LAG(): Accede a la fila anterior en la ventana.
        Uso: obtener el precio del dÃ­a anterior para calcular % cambio.

    AVG() OVER (ROWS BETWEEN N PRECEDING AND CURRENT ROW):
        Media mÃ³vil de los Ãºltimos N+1 dÃ­as.
        Uso: Moving Average 7d y 30d (indicadores tÃ©cnicos clÃ¡sicos).

    STDDEV() OVER (...):
        DesviaciÃ³n estÃ¡ndar sobre la ventana.
        Uso: Volatilidad â€” cuÃ¡nto varÃ­a el precio.

    Todas las ventanas se particionan por coin_id y ordenan por price_date.
    Esto significa que los cÃ¡lculos son INDEPENDIENTES por cada moneda.
    """
    print("\nğŸ“Š Construyendo fact_market_daily...")

    # Primero necesitamos leer Silver y registrar como vista temporal
    prices = spark.table("cryptolake.silver.daily_prices")
    fear_greed = spark.table("cryptolake.silver.fear_greed")

    prices.createOrReplaceTempView("s_prices")
    fear_greed.createOrReplaceTempView("s_fear_greed")

    spark.sql("""
        CREATE OR REPLACE TABLE cryptolake.gold.fact_market_daily
        USING iceberg
        PARTITIONED BY (coin_id)
        LOCATION 's3://cryptolake-gold/fact_market_daily'
        AS
        WITH price_metrics AS (
            SELECT
                p.coin_id,
                p.price_date,
                p.price_usd,
                p.market_cap_usd,
                p.volume_24h_usd,
                
                -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                -- PRICE CHANGE % (dÃ­a sobre dÃ­a)
                -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                -- LAG(price_usd, 1) devuelve el precio del dÃ­a anterior.
                -- FÃ³rmula: ((precio_hoy - precio_ayer) / precio_ayer) Ã— 100
                ROUND(
                    (p.price_usd - LAG(p.price_usd, 1) OVER w_coin)
                    / LAG(p.price_usd, 1) OVER w_coin * 100,
                    4
                ) AS price_change_pct_1d,
                
                -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                -- MOVING AVERAGES (medias mÃ³viles)
                -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                -- MA7: Media de los Ãºltimos 7 dÃ­as (6 anteriores + hoy)
                -- Se usa como indicador de tendencia a corto plazo.
                ROUND(
                    AVG(p.price_usd) OVER (
                        PARTITION BY p.coin_id ORDER BY p.price_date
                        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
                    ),
                    6
                ) AS moving_avg_7d,
                
                -- MA30: Media de los Ãºltimos 30 dÃ­as
                -- Indicador de tendencia a medio plazo.
                -- Cuando el precio cruza por encima de MA30 = seÃ±al alcista.
                ROUND(
                    AVG(p.price_usd) OVER (
                        PARTITION BY p.coin_id ORDER BY p.price_date
                        ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
                    ),
                    6
                ) AS moving_avg_30d,
                
                -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                -- VOLATILIDAD (desviaciÃ³n estÃ¡ndar 7d)
                -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                -- Alta volatilidad = mucho riesgo/oportunidad.
                -- Bitcoin tÃ­picamente: 2-5% diario.
                -- Altcoins: 5-15% diario.
                ROUND(
                    STDDEV(p.price_usd) OVER (
                        PARTITION BY p.coin_id ORDER BY p.price_date
                        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
                    ),
                    6
                ) AS volatility_7d,
                
                -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                -- VOLUME TREND (media de volumen 7d)
                -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                ROUND(
                    AVG(p.volume_24h_usd) OVER (
                        PARTITION BY p.coin_id ORDER BY p.price_date
                        ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
                    ),
                    2
                ) AS avg_volume_7d
                
            FROM s_prices p
            WINDOW w_coin AS (PARTITION BY p.coin_id ORDER BY p.price_date)
        )
        
        -- JOIN con Fear & Greed y aÃ±adir seÃ±ales
        SELECT
            pm.coin_id,
            pm.price_date,
            pm.price_usd,
            pm.market_cap_usd,
            pm.volume_24h_usd,
            pm.price_change_pct_1d,
            pm.moving_avg_7d,
            pm.moving_avg_30d,
            pm.volatility_7d,
            pm.avg_volume_7d,
            
            -- Fear & Greed del dÃ­a
            fg.fear_greed_value,
            fg.classification AS market_sentiment,
            
            -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            -- SEÃ‘AL MA30
            -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            -- Si el precio estÃ¡ por encima de la media de 30 dÃ­as,
            -- la tendencia general es alcista. Si estÃ¡ por debajo,
            -- es bajista. Es uno de los indicadores mÃ¡s bÃ¡sicos
            -- pero mÃ¡s usados en trading.
            CASE
                WHEN pm.price_usd > pm.moving_avg_30d THEN 'ABOVE_MA30'
                WHEN pm.price_usd < pm.moving_avg_30d THEN 'BELOW_MA30'
                ELSE 'AT_MA30'
            END AS ma30_signal,
            
            -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            -- SEÃ‘AL COMBINADA (precio + sentimiento)
            -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            -- Combina el indicador tÃ©cnico (MA30) con el sentimiento
            -- del mercado. "Extreme Fear + BELOW_MA30" podrÃ­a ser
            -- oportunidad de compra segÃºn la filosofÃ­a contrarian.
            CASE
                WHEN pm.price_usd < pm.moving_avg_30d
                     AND fg.fear_greed_value < 25
                THEN 'POTENTIAL_BUY'
                WHEN pm.price_usd > pm.moving_avg_30d
                     AND fg.fear_greed_value > 75
                THEN 'POTENTIAL_SELL'
                ELSE 'HOLD'
            END AS combined_signal,
            
            current_timestamp() AS _loaded_at
            
        FROM price_metrics pm
        LEFT JOIN s_fear_greed fg
            ON pm.price_date = fg.index_date
    """)

    count = spark.table("cryptolake.gold.fact_market_daily").count()
    print(f"  âœ… fact_market_daily: {count} registros")


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ CryptoLake â€” Silver to Gold (Star Schema)")
    print("=" * 60)

    spark = SparkSession.builder.appName("CryptoLake-SilverToGold").getOrCreate()

    try:
        spark.sql("CREATE NAMESPACE IF NOT EXISTS cryptolake.gold LOCATION 's3://cryptolake-gold/'")

        build_dim_coins(spark)
        build_dim_dates(spark)
        build_fact_market_daily(spark)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # VERIFICACIÃ“N DEL STAR SCHEMA
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        print("\n" + "=" * 60)
        print("ğŸ“‹ VERIFICACIÃ“N GOLD â€” Star Schema")
        print("=" * 60)

        # dim_coins
        print("\nâ”€â”€ dim_coins â”€â”€")
        spark.sql("""
            SELECT coin_id, total_days_tracked,
                   all_time_low, all_time_high, price_range_pct
            FROM cryptolake.gold.dim_coins
            ORDER BY price_range_pct DESC
        """).show(truncate=False)

        # dim_dates sample
        print("â”€â”€ dim_dates (muestra) â”€â”€")
        spark.sql("""
            SELECT date_day, day_name, month_name, quarter, is_weekend
            FROM cryptolake.gold.dim_dates
            ORDER BY date_day DESC
            LIMIT 5
        """).show(truncate=False)

        # fact_market_daily â€” query analÃ­tica de ejemplo
        print("â”€â”€ fact_market_daily: Bitcoin Ãºltimos 7 dÃ­as â”€â”€")
        spark.sql("""
            SELECT price_date, 
                   ROUND(price_usd, 2) as price,
                   price_change_pct_1d as change_pct,
                   ROUND(moving_avg_7d, 2) as ma7,
                   ROUND(moving_avg_30d, 2) as ma30,
                   ma30_signal,
                   market_sentiment,
                   combined_signal
            FROM cryptolake.gold.fact_market_daily
            WHERE coin_id = 'bitcoin'
            ORDER BY price_date DESC
            LIMIT 7
        """).show(truncate=False)

        # Query analÃ­tica avanzada: Â¿QuÃ© coins tienen seÃ±al de compra?
        print("â”€â”€ SeÃ±ales de compra potenciales (Ãºltimos datos) â”€â”€")
        spark.sql("""
            WITH latest AS (
                SELECT *, ROW_NUMBER() OVER (
                    PARTITION BY coin_id ORDER BY price_date DESC
                ) as rn
                FROM cryptolake.gold.fact_market_daily
            )
            SELECT coin_id, price_date,
                   ROUND(price_usd, 4) as price,
                   price_change_pct_1d,
                   ma30_signal,
                   market_sentiment,
                   combined_signal
            FROM latest
            WHERE rn = 1
            ORDER BY combined_signal, coin_id
        """).show(truncate=False)

    finally:
        spark.stop()

    print("\nâœ… Gold (Star Schema) completado!")
