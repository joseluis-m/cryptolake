# CryptoLake â€” Fase 5 y Fase 6

## GuÃ­a paso a paso: dbt + Airflow

> **Resumen**: En la Fase 5 aÃ±adimos dbt (data build tool) para crear la capa Gold de forma
> declarativa, testable y documentada. En la Fase 6 orquestamos todo el pipeline con Apache
> Airflow para que se ejecute automÃ¡ticamente cada dÃ­a.

---

## PARTE 7 (FASE 5): Gold Layer con dbt

### 7.1 â€” Conceptos fundamentales

**Â¿QuÃ© es dbt?**

dbt (data build tool) es una herramienta que te permite transformar datos escribiendo solo
SQL. En lugar de programar un script PySpark completo con imports, SparkSession, funciones,
etc., simplemente escribes la consulta SQL y dbt se encarga de:

- Crear las tablas/vistas por ti (ejecuta `CREATE TABLE ... AS SELECT ...`)
- Gestionar dependencias entre modelos (si `fact_market_daily` necesita datos de `stg_prices`,
  dbt se asegura de ejecutar `stg_prices` primero)
- Ejecutar tests automÃ¡ticos sobre los datos (Â¿hay nulls? Â¿hay duplicados?)
- Generar documentaciÃ³n automÃ¡tica de tu data warehouse

**Â¿Por quÃ© dbt si ya tenemos Spark?**

En la Fase 4 construiste la Gold layer con PySpark puro (`silver_to_gold.py`). Funcionaba, pero
tenÃ­as que escribir mucho cÃ³digo Python para cosas que son simplemente SQL. dbt es el estÃ¡ndar
de la industria para la "T" de ELT (Extract-Load-Transform). En entrevistas, saber dbt es un
diferenciador enorme.

La clave: **Spark procesa** (Bronze â†’ Silver, deduplicaciÃ³n, limpieza pesada), **dbt modela**
(Silver â†’ Gold, star schema, mÃ©tricas de negocio). Cada herramienta para lo que hace mejor.

**Â¿CÃ³mo conecta dbt con Spark?**

dbt necesita una forma de enviar SQL a Spark. Para eso usamos el **Spark Thrift Server**
(tambiÃ©n llamado HiveServer2). Es un servicio que expone Spark SQL como un servidor JDBC al que
dbt se conecta, envÃ­a consultas SQL, y Spark las ejecuta sobre las tablas Iceberg.

```
dbt (SQL) â”€â”€JDBCâ”€â”€â–¶ Spark Thrift Server â”€â”€â–¶ Spark SQL â”€â”€â–¶ Iceberg Tables (MinIO)
```

**Estructura de un proyecto dbt:**

```
dbt_cryptolake/
â”œâ”€â”€ dbt_project.yml       â† ConfiguraciÃ³n del proyecto (nombre, carpetas, materializaciÃ³n)
â”œâ”€â”€ profiles.yml          â† ConexiÃ³n al servidor (host, puerto, schema)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ sources.yml       â† Define de dÃ³nde lee dbt (tablas Silver en Iceberg)
â”‚   â”œâ”€â”€ staging/          â† Capa de interfaz: renombra, limpia, calcula campos bÃ¡sicos
â”‚   â”‚   â”œâ”€â”€ stg_prices.sql
â”‚   â”‚   â””â”€â”€ stg_fear_greed.sql
â”‚   â””â”€â”€ marts/            â† Capa de negocio: star schema (dimensiones + facts)
â”‚       â”œâ”€â”€ dim_coins.sql
â”‚       â”œâ”€â”€ dim_dates.sql
â”‚       â”œâ”€â”€ fact_market_daily.sql
â”‚       â””â”€â”€ schema.yml    â† Tests y documentaciÃ³n de los modelos
â”œâ”€â”€ macros/               â† Funciones SQL reutilizables (personalizaciones)
â”‚   â”œâ”€â”€ generate_schema_name.sql
â”‚   â””â”€â”€ create_table_as.sql
â””â”€â”€ tests/                â† Tests SQL personalizados
    â””â”€â”€ assert_positive_prices.sql
```

**MaterializaciÃ³n** â€” cÃ³mo dbt crea cada modelo:

- `view` (staging): Crea una vista SQL. No almacena datos, es solo una consulta guardada.
  Ideal para staging porque se actualiza siempre con los datos mÃ¡s recientes.
- `table` (dimensiones): Crea una tabla fÃ­sica. Se recrea completa en cada ejecuciÃ³n.
  Ideal para dimensiones que son pequeÃ±as.
- `incremental` (facts): Solo procesa los datos nuevos y los aÃ±ade/actualiza. Mucho mÃ¡s
  eficiente para tablas grandes que crecen cada dÃ­a.
- `ephemeral`: No crea nada en la base de datos. Se convierte en un CTE que se incrusta
  en los modelos que lo usan. Ãštil para transformaciones intermedias.

---

### 7.2 â€” AÃ±adir Spark Thrift Server al Docker Compose

El Thrift Server es un proceso Spark que escucha conexiones JDBC en el puerto 10000.
Usa la misma configuraciÃ³n de Iceberg que el Spark Master, asÃ­ que puede leer las tablas
Bronze y Silver directamente.

Abre `docker-compose.yml` y aÃ±ade este servicio **despuÃ©s del bloque `spark-worker`** y
**antes de `airflow-postgres`**:

```yaml
  # ============================================================
  # Spark Thrift Server: Punto de entrada JDBC para dbt y SQL tools.
  # Permite ejecutar consultas SQL sobre Iceberg desde fuera de Spark.
  # Es como un "puente" entre herramientas SQL (dbt, DBeaver, etc.)
  # y el motor de procesamiento Spark.
  # ============================================================
  spark-thrift:
    image: cryptolake-spark
    container_name: cryptolake-spark-thrift
    ports:
      - "10000:10000"      # Puerto JDBC â€” dbt se conecta aquÃ­
    environment:
      # Sin este flag, el proceso arranca en background y Docker lo mata
      SPARK_NO_DAEMONIZE: "true"
    volumes:
      # Montamos el cÃ³digo fuente para que Spark pueda acceder a los scripts
      - ./src:/opt/spark/work/src:ro
      # ConfiguraciÃ³n de Spark con Iceberg
      - ./spark-defaults.conf:/opt/spark/conf/spark-defaults.conf:ro
    depends_on:
      spark-master:
        condition: service_healthy
    # Arranca el Thrift Server conectado al Spark Master.
    # --hiveconf configura el puerto y la direcciÃ³n de escucha.
    command: >
      /opt/spark/sbin/start-thriftserver.sh
        --master spark://spark-master:7077
        --hiveconf hive.server2.thrift.port=10000
        --hiveconf hive.server2.thrift.bind.host=0.0.0.0
    networks:
      - default
```

**Importante**: Este servicio usa la imagen `cryptolake-spark` (la que ya construyes con tu
Dockerfile de Spark). Pero para que Docker la encuentre por nombre, necesitas aÃ±adir
`image: cryptolake-spark` al servicio `spark-master` tambiÃ©n. Busca el bloque del `spark-master`
en tu docker-compose.yml y aÃ±ade la lÃ­nea `image`:

```yaml
  spark-master:
    build:
      context: ./docker/spark
      dockerfile: Dockerfile
    image: cryptolake-spark          # â† AÃ‘ADE esta lÃ­nea
    container_name: cryptolake-spark-master
    # ... resto igual ...
```

Esto hace que cuando Docker construya la imagen del spark-master, la etiquete como
`cryptolake-spark`, y el servicio `spark-thrift` pueda reutilizarla sin reconstruir.

Ahora reconstruye y arranca:

```bash
docker compose down
docker compose up -d --build
```

Verifica que el Thrift Server estÃ¡ corriendo:

```bash
docker logs cryptolake-spark-thrift 2>&1 | tail -5
```

DeberÃ­as ver algo como `ThriftCLIService started on port 10000`. TambiÃ©n puedes verificar
que el puerto estÃ¡ escuchando:

```bash
# Desde tu Mac, comprueba que el puerto 10000 estÃ¡ abierto
nc -zv localhost 10000
```

---

### 7.3 â€” Instalar dbt-spark en tu entorno local

dbt necesita el paquete `dbt-spark` con el conector PyHive (que habla el protocolo
Thrift/JDBC). Activa tu entorno virtual e instala:

```bash
cd ~/Projects/cryptolake

# Activa el entorno virtual
source .venv/bin/activate

# Instala dbt-spark con el conector PyHive
pip install "dbt-spark[PyHive]==1.8.0"
```

Esto instala `dbt-core`, `dbt-spark`, `PyHive`, y todas sus dependencias.

Verifica la instalaciÃ³n:

```bash
dbt --version
```

DeberÃ­as ver algo como:
```
Core:
  - installed: 1.8.x
Plugins:
  - spark: 1.8.0
```

---

### 7.4 â€” Crear la estructura del proyecto dbt

```bash
# Crear la carpeta del proyecto dbt dentro de src/transformation/
mkdir -p src/transformation/dbt_cryptolake/{models/{staging,marts},macros,tests}
```

Tu estructura deberÃ­a quedar:

```
src/transformation/dbt_cryptolake/
â”œâ”€â”€ dbt_project.yml         (lo creamos ahora)
â”œâ”€â”€ profiles.yml            (lo creamos ahora)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ sources.yml         (lo creamos ahora)
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ stg_prices.sql
â”‚   â”‚   â””â”€â”€ stg_fear_greed.sql
â”‚   â””â”€â”€ marts/
â”‚       â”œâ”€â”€ dim_coins.sql
â”‚       â”œâ”€â”€ dim_dates.sql
â”‚       â”œâ”€â”€ fact_market_daily.sql
â”‚       â””â”€â”€ schema.yml
â”œâ”€â”€ macros/
â”‚   â”œâ”€â”€ generate_schema_name.sql
â”‚   â””â”€â”€ create_table_as.sql
â””â”€â”€ tests/
    â””â”€â”€ assert_positive_prices.sql
```

---

### 7.5 â€” Configurar dbt_project.yml y profiles.yml

**dbt_project.yml** â€” la configuraciÃ³n central del proyecto:

```bash
cat > src/transformation/dbt_cryptolake/dbt_project.yml << 'EOF'
# ============================================================
# dbt_project.yml â€” ConfiguraciÃ³n del proyecto dbt
# ============================================================
# Este archivo define:
# - Nombre y versiÃ³n del proyecto
# - DÃ³nde buscar modelos, tests, macros, etc.
# - CÃ³mo materializar cada carpeta de modelos
#
# DocumentaciÃ³n: https://docs.getdbt.com/reference/dbt_project.yml
# ============================================================

name: cryptolake
version: '1.0.0'
config-version: 2

# "profile" conecta este proyecto con una configuraciÃ³n de conexiÃ³n
# en profiles.yml. Debe coincidir exactamente.
profile: cryptolake

# Rutas donde dbt busca cada tipo de archivo
model-paths: ["models"]
test-paths: ["tests"]
macro-paths: ["macros"]
seed-paths: ["seeds"]
analysis-paths: ["analyses"]

# Carpetas que dbt limpia con "dbt clean"
clean-targets:
  - target        # Carpeta donde dbt genera los SQL compilados
  - dbt_packages  # Paquetes externos instalados

# ============================================================
# ConfiguraciÃ³n de materializaciÃ³n por carpeta
# ============================================================
# Cada carpeta dentro de models/ tiene su propia estrategia.
# El prefijo "+" significa "aplica a todos los modelos en esta carpeta".
models:
  cryptolake:
    # staging/ â†’ Vistas SQL (no almacenan datos, siempre frescas)
    staging:
      +materialized: view
      +schema: staging

    # marts/ â†’ Tablas fÃ­sicas (star schema para anÃ¡lisis)
    marts:
      +materialized: table
      +schema: gold

# ============================================================
# Hooks que se ejecutan al inicio de cada "dbt run"
# ============================================================
# Creamos los namespaces de Iceberg si no existen.
# Sin esto, dbt fallarÃ­a al intentar crear tablas en un namespace
# que no existe.
on-run-start:
  - "CREATE NAMESPACE IF NOT EXISTS staging"
  - "CREATE NAMESPACE IF NOT EXISTS gold"
EOF
```

**profiles.yml** â€” la conexiÃ³n al Spark Thrift Server:

```bash
cat > src/transformation/dbt_cryptolake/profiles.yml << 'EOF'
# ============================================================
# profiles.yml â€” ConfiguraciÃ³n de conexiÃ³n de dbt
# ============================================================
# Define CÃ“MO dbt se conecta a Spark Thrift Server.
# Hay dos "targets" (entornos):
#   - dev:  desde tu Mac (localhost:10000)
#   - prod: desde dentro de Docker (spark-thrift:10000)
#
# El target "dev" es el que usarÃ¡s ahora para desarrollo.
# El target "prod" lo usarÃ¡ Airflow en la Fase 6.
# ============================================================

cryptolake:
  target: dev

  outputs:
    # Desarrollo: ejecutas dbt desde tu Mac
    dev:
      type: spark
      method: thrift
      host: localhost
      port: 10000
      schema: gold
      # threads: cuÃ¡ntas consultas SQL ejecuta dbt en paralelo.
      # 1 es seguro para desarrollo local.
      threads: 1

    # ProducciÃ³n: Airflow ejecuta dbt desde dentro de Docker
    prod:
      type: spark
      method: thrift
      host: spark-thrift
      port: 10000
      schema: gold
      threads: 2
EOF
```

Verifica que dbt puede conectar:

```bash
cd src/transformation/dbt_cryptolake
dbt debug --profiles-dir .
```

DeberÃ­as ver `All checks passed!` al final. Si falla la conexiÃ³n, asegÃºrate de que
`cryptolake-spark-thrift` estÃ¡ corriendo (`docker ps | grep thrift`).

---

### 7.6 â€” Crear sources.yml (origen de datos)

Los "sources" le dicen a dbt dÃ³nde estÃ¡n los datos de entrada. En nuestro caso, las tablas
Silver de Iceberg que creamos en la Fase 4.

```bash
cat > src/transformation/dbt_cryptolake/models/sources.yml << 'EOF'
# ============================================================
# sources.yml â€” DefiniciÃ³n de datos de entrada para dbt
# ============================================================
# Un "source" en dbt es una tabla que ya existe en tu base de datos
# y que dbt NO crea ni gestiona. Solo la lee.
#
# En nuestro caso, las tablas Silver son creadas por los scripts
# de Spark (bronze_to_silver.py). dbt las lee para construir Gold.
#
# En los modelos SQL, las referenciamos asÃ­:
#   {{ source('silver', 'daily_prices') }}
#   â†’ se traduce a: silver.daily_prices
#   â†’ que Spark resuelve como: cryptolake.silver.daily_prices
# ============================================================

version: 2

sources:
  - name: silver
    description: "Capa Silver del Lakehouse â€” datos limpios y deduplicados"
    schema: silver
    tables:
      - name: daily_prices
        description: "Precios diarios por criptomoneda, deduplicados y validados"
        columns:
          - name: coin_id
            description: "Identificador Ãºnico de la criptomoneda (ej: bitcoin, ethereum)"
          - name: price_date
            description: "Fecha del precio (tipo DATE)"
          - name: price_usd
            description: "Precio en dÃ³lares estadounidenses"
          - name: market_cap_usd
            description: "CapitalizaciÃ³n de mercado en USD"
          - name: volume_24h_usd
            description: "Volumen de trading en las Ãºltimas 24 horas"
          - name: _processed_at
            description: "Timestamp de cuÃ¡ndo se procesÃ³ este registro en Silver"

      - name: fear_greed
        description: "Ãndice Fear & Greed del mercado crypto (0-100)"
        columns:
          - name: index_date
            description: "Fecha del Ã­ndice"
          - name: fear_greed_value
            description: "Valor numÃ©rico (0=Extreme Fear, 100=Extreme Greed)"
          - name: classification
            description: "ClasificaciÃ³n textual del sentimiento"
          - name: _processed_at
            description: "Timestamp de procesamiento"
EOF
```

---

### 7.7 â€” Crear macros personalizadas

Necesitamos dos macros. La primera controla cÃ³mo dbt nombra los schemas. La segunda
resuelve el problema de LOCATION que vimos en la Fase 4 (que cada tabla acabe en su
bucket correcto).

**Macro 1: generate_schema_name**

Por defecto, dbt genera schemas como `{target_schema}_{custom_schema}`, es decir, si tu
target es `gold` y el modelo tiene `+schema: gold`, generarÃ­a `gold_gold`. Queremos que
use solo el `custom_schema` directamente.

```bash
cat > src/transformation/dbt_cryptolake/macros/generate_schema_name.sql << 'SQLEOF'
{#
  ============================================================
  Macro: generate_schema_name
  ============================================================
  Sobreescribe el comportamiento por defecto de dbt para
  nombrar schemas.

  Comportamiento por defecto de dbt:
    target.schema = "gold", custom_schema = "staging"
    â†’ genera: "gold_staging"  (Â¡no es lo que queremos!)

  Nuestro comportamiento:
    custom_schema = "staging" â†’ genera: "staging"
    custom_schema = null      â†’ genera: target.schema ("gold")

  Esto hace que las tablas de staging vayan al namespace
  "cryptolake.staging" y las de marts al namespace "cryptolake.gold".
  ============================================================
#}

{% macro generate_schema_name(custom_schema_name, node) -%}
    {%- if custom_schema_name is none -%}
        {{ target.schema }}
    {%- else -%}
        {{ custom_schema_name | trim }}
    {%- endif -%}
{%- endmacro %}
SQLEOF
```

**Macro 2: create_table_as (LOCATION para Iceberg)**

Esta macro sobreescribe cÃ³mo dbt crea tablas en Spark. AÃ±ade `LOCATION` para que
cada tabla aterrice en su bucket correcto de MinIO.

```bash
cat > src/transformation/dbt_cryptolake/macros/create_table_as.sql << 'SQLEOF'
{#
  ============================================================
  Macro: create_table_as (override para Spark + Iceberg)
  ============================================================
  Problema: El catÃ¡logo REST de Iceberg (tabulario) ignora el
  LOCATION del namespace. Sin esta macro, todas las tablas que
  dbt crea irÃ­an al bucket por defecto (cryptolake-bronze).

  SoluciÃ³n: Inyectamos LOCATION explÃ­cito en cada CREATE TABLE
  apuntando al bucket correcto basÃ¡ndonos en el schema del modelo.

  Ejemplo para un modelo en schema "gold":
    CREATE OR REPLACE TABLE gold.dim_coins
    USING iceberg
    LOCATION 's3://cryptolake-gold/dim_coins'
    AS SELECT ...
  ============================================================
#}

{% macro spark__create_table_as(temporary, relation, compiled_code) -%}
  {# Construimos el nombre del bucket a partir del schema #}
  {%- set bucket = 'cryptolake-' ~ relation.schema -%}

  create or replace table {{ relation }}
  using iceberg
  location 's3://{{ bucket }}/{{ relation.identifier }}'
  as
  {{ compiled_code }}
{%- endmacro %}
SQLEOF
```

---

### 7.8 â€” Crear modelos Staging

Los modelos staging son la "interfaz limpia" sobre los datos Silver. Renombran columnas
si es necesario, aplican lÃ³gica de negocio mÃ­nima, y calculan campos bÃ¡sicos. Son vistas,
no tablas, asÃ­ que no almacenan datos.

**stg_prices.sql:**

```bash
cat > src/transformation/dbt_cryptolake/models/staging/stg_prices.sql << 'SQLEOF'
-- ============================================================
-- Staging: stg_prices
-- ============================================================
-- Interfaz limpia sobre silver.daily_prices.
--
-- AÃ±ade dos campos calculados:
--   - prev_day_price: precio del dÃ­a anterior (con LAG window function)
--   - price_change_pct_1d: cambio porcentual respecto al dÃ­a anterior
--
-- LAG() es una window function que "mira hacia atrÃ¡s" en los datos:
--   LAG(columna) OVER (PARTITION BY grupo ORDER BY orden)
--   = "dame el valor de 'columna' en la fila anterior dentro del mismo 'grupo'"
--
-- Ejemplo con Bitcoin:
--   Fecha       Precio    LAG(precio)  Change %
--   2024-01-01  42000     NULL         NULL (no hay dÃ­a anterior)
--   2024-01-02  43000     42000        +2.38%
--   2024-01-03  41500     43000        -3.49%
-- ============================================================

WITH source AS (
    SELECT * FROM {{ source('silver', 'daily_prices') }}
),

with_lag AS (
    SELECT
        coin_id,
        price_date,
        price_usd,
        market_cap_usd,
        volume_24h_usd,
        _processed_at,

        -- LAG: precio del dÃ­a anterior para esta misma moneda
        LAG(price_usd) OVER (
            PARTITION BY coin_id ORDER BY price_date
        ) AS prev_day_price

    FROM source
    WHERE price_usd > 0
)

SELECT
    *,
    -- CÃ¡lculo del cambio porcentual dÃ­a a dÃ­a
    -- FÃ³rmula: ((nuevo - anterior) / anterior) * 100
    CASE
        WHEN prev_day_price IS NOT NULL AND prev_day_price > 0
        THEN ROUND(((price_usd - prev_day_price) / prev_day_price) * 100, 4)
        ELSE NULL
    END AS price_change_pct_1d
FROM with_lag
SQLEOF
```

**stg_fear_greed.sql:**

```bash
cat > src/transformation/dbt_cryptolake/models/staging/stg_fear_greed.sql << 'SQLEOF'
-- ============================================================
-- Staging: stg_fear_greed
-- ============================================================
-- Interfaz limpia sobre silver.fear_greed.
--
-- AÃ±ade un sentiment_score numÃ©rico para facilitar el anÃ¡lisis.
-- Es mÃ¡s fÃ¡cil hacer AVG(sentiment_score) que contar strings.
--
-- ClasificaciÃ³n:
--   "Extreme Fear"  â†’ 1  (pÃ¡nico en el mercado)
--   "Fear"          â†’ 2
--   "Neutral"       â†’ 3
--   "Greed"         â†’ 4
--   "Extreme Greed" â†’ 5  (euforia, posible burbuja)
-- ============================================================

WITH source AS (
    SELECT * FROM {{ source('silver', 'fear_greed') }}
)

SELECT
    index_date,
    fear_greed_value,
    classification,

    -- Convertimos la clasificaciÃ³n textual a un score numÃ©rico
    CASE classification
        WHEN 'Extreme Fear' THEN 1
        WHEN 'Fear' THEN 2
        WHEN 'Neutral' THEN 3
        WHEN 'Greed' THEN 4
        WHEN 'Extreme Greed' THEN 5
    END AS sentiment_score,

    _processed_at

FROM source
SQLEOF
```

---

### 7.9 â€” Crear modelos Marts (Star Schema)

AquÃ­ creamos el star schema dimensional (Kimball). Este es el modelo que los analistas
y dashboards consultarÃ¡n. Tiene tres tablas: dos dimensiones y una fact table.

**dim_coins.sql** â€” DimensiÃ³n con estadÃ­sticas de cada criptomoneda:

```bash
cat > src/transformation/dbt_cryptolake/models/marts/dim_coins.sql << 'SQLEOF'
-- ============================================================
-- DimensiÃ³n: dim_coins
-- ============================================================
-- Contiene una fila por criptomoneda con estadÃ­sticas agregadas.
--
-- Tipo: SCD Type 1 (Slowly Changing Dimension Type 1)
-- â†’ Cuando los datos cambian, sobrescribimos. No guardamos historial.
-- â†’ Se recrea completa en cada ejecuciÃ³n (materialized: table).
--
-- Ejemplo de resultado:
--   coin_id  | first_tracked_date | all_time_high | avg_price | ...
--   bitcoin  | 2024-11-14         | 106000.0      | 95234.5   | ...
--   ethereum | 2024-11-14         | 3800.0        | 3256.7    | ...
-- ============================================================

{{ config(
    materialized='table',
    unique_key='coin_id'
) }}

WITH coin_stats AS (
    SELECT
        coin_id,

        -- Rango de fechas en que tenemos datos de este coin
        MIN(price_date) AS first_tracked_date,
        MAX(price_date) AS last_tracked_date,
        COUNT(DISTINCT price_date) AS total_days_tracked,

        -- EstadÃ­sticas de precio
        MIN(price_usd) AS all_time_low,
        MAX(price_usd) AS all_time_high,
        ROUND(AVG(price_usd), 2) AS avg_price,

        -- EstadÃ­sticas de volumen
        ROUND(AVG(volume_24h_usd), 2) AS avg_daily_volume

    FROM {{ ref('stg_prices') }}
    GROUP BY coin_id
)

SELECT
    coin_id,
    first_tracked_date,
    last_tracked_date,
    total_days_tracked,
    all_time_low,
    all_time_high,
    avg_price,
    avg_daily_volume,

    -- Rango de precio en porcentaje: cuÃ¡nto variÃ³ entre mÃ­nimo y mÃ¡ximo
    ROUND(((all_time_high - all_time_low) / all_time_low) * 100, 2) AS price_range_pct,

    CURRENT_TIMESTAMP() AS _loaded_at

FROM coin_stats
SQLEOF
```

**dim_dates.sql** â€” DimensiÃ³n calendario:

```bash
cat > src/transformation/dbt_cryptolake/models/marts/dim_dates.sql << 'SQLEOF'
-- ============================================================
-- DimensiÃ³n: dim_dates
-- ============================================================
-- Calendario con atributos Ãºtiles para anÃ¡lisis temporal.
--
-- Â¿Por quÃ© una tabla de fechas separada?
-- En un star schema, las dimensiones de fecha permiten filtrar y agrupar
-- fÃ¡cilmente: "ventas del Q1", "solo dÃ­as laborables", "por mes", etc.
-- Sin esta tabla, tendrÃ­as que calcular YEAR(), MONTH(), etc. en cada
-- consulta. Con ella, solo haces un JOIN y filtras.
--
-- Ejemplo de resultado:
--   date_day   | year | month | quarter | is_weekend | day_name
--   2024-11-14 | 2024 | 11    | 4       | false      | Thursday
--   2024-11-15 | 2024 | 11    | 4       | false      | Friday
--   2024-11-16 | 2024 | 11    | 4       | true       | Saturday
-- ============================================================

{{ config(materialized='table') }}

WITH date_spine AS (
    -- Extraemos todas las fechas Ãºnicas de nuestros datos de precios
    SELECT DISTINCT price_date AS date_day
    FROM {{ ref('stg_prices') }}
)

SELECT
    date_day,
    YEAR(date_day) AS year,
    MONTH(date_day) AS month,
    DAY(date_day) AS day_of_month,
    DAYOFWEEK(date_day) AS day_of_week,
    WEEKOFYEAR(date_day) AS week_of_year,
    QUARTER(date_day) AS quarter,

    -- Flag de fin de semana (Ãºtil para anÃ¡lisis de volumen)
    -- Crypto opera 24/7, pero el volumen suele bajar los fines de semana
    CASE WHEN DAYOFWEEK(date_day) IN (1, 7) THEN TRUE ELSE FALSE END AS is_weekend,

    -- Nombres legibles para dashboards
    DATE_FORMAT(date_day, 'EEEE') AS day_name,
    DATE_FORMAT(date_day, 'MMMM') AS month_name

FROM date_spine
SQLEOF
```

**fact_market_daily.sql** â€” Tabla de hechos central:

```bash
cat > src/transformation/dbt_cryptolake/models/marts/fact_market_daily.sql << 'SQLEOF'
-- ============================================================
-- Fact Table: fact_market_daily
-- ============================================================
-- Tabla de hechos central del star schema.
-- Granularidad: 1 fila = 1 moneda Ã— 1 dÃ­a
--
-- Contiene:
--   - Datos base: precio, market cap, volumen
--   - MÃ©tricas calculadas con window functions:
--     Â· Moving averages (7d, 30d)
--     Â· Volatilidad 7 dÃ­as
--     Â· SeÃ±al MA30 (above/below)
--   - Datos de sentimiento del mercado (Fear & Greed)
--
-- Window Functions usadas:
-- â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
-- AVG(...) OVER (PARTITION BY coin ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)
--
--   PARTITION BY coin: calcula por separado para cada moneda
--   ORDER BY date: ordena cronolÃ³gicamente
--   ROWS BETWEEN 6 PRECEDING AND CURRENT ROW: ventana de 7 dÃ­as
--                                              (6 anteriores + el actual)
--
-- Ejemplo visual para Bitcoin, ventana de 3 dÃ­as:
--   Fecha       Precio    AVG(ventana 3d)
--   2024-01-01  42000     42000        â† solo 1 dÃ­a disponible
--   2024-01-02  43000     42500        â† promedio de 2 dÃ­as
--   2024-01-03  41500     42166.67     â† promedio de 3 dÃ­as (42000+43000+41500)/3
--   2024-01-04  44000     42833.33     â† ventana se "desliza" (43000+41500+44000)/3
-- ============================================================

{{ config(
    materialized='table',
    unique_key=['coin_id', 'price_date']
) }}

WITH prices AS (
    SELECT * FROM {{ ref('stg_prices') }}
),

fear_greed AS (
    SELECT * FROM {{ ref('stg_fear_greed') }}
),

-- Calculamos todas las mÃ©tricas con window functions
enriched AS (
    SELECT
        p.coin_id,
        p.price_date,
        p.price_usd,
        p.market_cap_usd,
        p.volume_24h_usd,
        p.price_change_pct_1d,

        -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        -- MEDIAS MÃ“VILES (Moving Averages)
        -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        -- Media de los Ãºltimos 7 dÃ­as. Suaviza el ruido diario.
        -- Si el precio estÃ¡ por encima de la MA7, tendencia alcista a corto plazo.
        ROUND(AVG(p.price_usd) OVER (
            PARTITION BY p.coin_id
            ORDER BY p.price_date
            ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
        ), 2) AS moving_avg_7d,

        -- Media de los Ãºltimos 30 dÃ­as. Indica tendencia de medio plazo.
        ROUND(AVG(p.price_usd) OVER (
            PARTITION BY p.coin_id
            ORDER BY p.price_date
            ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
        ), 2) AS moving_avg_30d,

        -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        -- VOLATILIDAD (desviaciÃ³n estÃ¡ndar 7 dÃ­as)
        -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        -- Mide cuÃ¡nto fluctÃºa el precio. Alta volatilidad = mÃ¡s riesgo.
        -- STDDEV calcula la dispersiÃ³n estadÃ­stica del precio.
        ROUND(STDDEV(p.price_usd) OVER (
            PARTITION BY p.coin_id
            ORDER BY p.price_date
            ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
        ), 2) AS volatility_7d,

        -- Media de volumen 7 dÃ­as (tendencia de actividad)
        ROUND(AVG(p.volume_24h_usd) OVER (
            PARTITION BY p.coin_id
            ORDER BY p.price_date
            ROWS BETWEEN 6 PRECEDING AND CURRENT ROW
        ), 2) AS avg_volume_7d,

        -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        -- SENTIMIENTO DEL MERCADO (Fear & Greed Index)
        -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        -- LEFT JOIN porque no todos los dÃ­as tienen dato de F&G
        fg.fear_greed_value,
        fg.classification AS market_sentiment,
        fg.sentiment_score,

        -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        -- SEÃ‘AL MA30: Â¿Precio por encima o debajo de la media 30d?
        -- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        -- SeÃ±al tÃ©cnica bÃ¡sica:
        --   ABOVE_MA30 = tendencia alcista (precio > media)
        --   BELOW_MA30 = tendencia bajista (precio < media)
        CASE
            WHEN p.price_usd > AVG(p.price_usd) OVER (
                PARTITION BY p.coin_id
                ORDER BY p.price_date
                ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
            ) THEN 'ABOVE_MA30'
            ELSE 'BELOW_MA30'
        END AS ma30_signal

    FROM prices p
    LEFT JOIN fear_greed fg
        ON p.price_date = fg.index_date
)

SELECT
    coin_id,
    price_date,
    price_usd,
    market_cap_usd,
    volume_24h_usd,
    price_change_pct_1d,
    moving_avg_7d,
    moving_avg_30d,
    volatility_7d,
    avg_volume_7d,
    fear_greed_value,
    market_sentiment,
    sentiment_score,
    ma30_signal,
    CURRENT_TIMESTAMP() AS _loaded_at
FROM enriched
SQLEOF
```

---

### 7.10 â€” Crear tests de datos

Los tests de dbt validan la calidad de los datos despuÃ©s de cada ejecuciÃ³n. Si un test
falla, dbt te lo dice y puedes investigar. Hay dos tipos de tests: genÃ©ricos (declarados
en YAML) y singulares (consultas SQL en la carpeta tests/).

**schema.yml** â€” Tests genÃ©ricos (declarativos):

```bash
cat > src/transformation/dbt_cryptolake/models/marts/schema.yml << 'SQLEOF'
# ============================================================
# schema.yml â€” Tests y documentaciÃ³n de los modelos marts
# ============================================================
# Los tests genÃ©ricos de dbt se declaran junto a cada columna.
# dbt genera y ejecuta la consulta SQL automÃ¡ticamente.
#
# Tipos de tests genÃ©ricos:
#   - not_null: Verifica que la columna no tiene valores NULL
#   - unique: Verifica que no hay duplicados
#   - accepted_values: Verifica que los valores estÃ¡n en una lista
#   - relationships: Verifica integridad referencial (FK)
#
# Un test PASA si la consulta devuelve 0 filas.
# Un test FALLA si la consulta devuelve 1 o mÃ¡s filas.
# ============================================================

version: 2

models:
  - name: fact_market_daily
    description: "Tabla de hechos con mÃ©tricas de mercado crypto diarias. Granularidad: 1 coin Ã— 1 dÃ­a."
    columns:
      - name: coin_id
        description: "Identificador de la criptomoneda"
        tests:
          - not_null
      - name: price_date
        description: "Fecha del registro"
        tests:
          - not_null
      - name: price_usd
        description: "Precio en USD"
        tests:
          - not_null

  - name: dim_coins
    description: "DimensiÃ³n con estadÃ­sticas agregadas por criptomoneda (SCD Type 1)"
    columns:
      - name: coin_id
        description: "Identificador Ãºnico de la criptomoneda"
        tests:
          - unique
          - not_null
      - name: all_time_high
        tests:
          - not_null
      - name: total_days_tracked
        tests:
          - not_null

  - name: dim_dates
    description: "DimensiÃ³n calendario con atributos temporales"
    columns:
      - name: date_day
        description: "Fecha (clave primaria)"
        tests:
          - unique
          - not_null
SQLEOF
```

**Test singular â€” assert_positive_prices.sql:**

```bash
cat > src/transformation/dbt_cryptolake/tests/assert_positive_prices.sql << 'SQLEOF'
-- ============================================================
-- Test singular: assert_positive_prices
-- ============================================================
-- Verifica que no hay precios negativos o cero en la fact table.
-- Un precio <= 0 indicarÃ­a datos corruptos o un error en la ingesta.
--
-- Si esta consulta devuelve filas, el test FALLA.
-- ============================================================

SELECT
    coin_id,
    price_date,
    price_usd
FROM {{ ref('fact_market_daily') }}
WHERE price_usd <= 0
SQLEOF
```

---

### 7.11 â€” Ejecutar dbt

Ahora vamos a ejecutar dbt y ver cÃ³mo crea las tablas Gold.

```bash
# AsegÃºrate de estar en la carpeta del proyecto dbt
cd ~/Projects/cryptolake/src/transformation/dbt_cryptolake

# 1. Verificar que la conexiÃ³n funciona
dbt debug --profiles-dir .

# 2. Ejecutar todos los modelos (staging â†’ marts)
dbt run --profiles-dir .

# 3. Ejecutar los tests
dbt test --profiles-dir .
```

**Â¿QuÃ© hace `dbt run`?**

1. Lee todos los archivos .sql en models/
2. Detecta las dependencias (fact_market_daily depende de stg_prices y stg_fear_greed)
3. Ejecuta en el orden correcto:
   - Primero: stg_prices y stg_fear_greed (vistas en namespace staging)
   - DespuÃ©s: dim_coins, dim_dates, fact_market_daily (tablas en namespace gold)
4. Cada modelo se ejecuta como SQL en el Spark Thrift Server

DeberÃ­as ver algo como:

```
Running with dbt=1.8.x
Found 5 models, 7 tests, 2 sources

Concurrency: 1 threads (target='dev')

1 of 5 START sql view model staging.stg_prices ................ [RUN]
1 of 5 OK created sql view model staging.stg_prices ........... [OK in 2.34s]
2 of 5 START sql view model staging.stg_fear_greed ............ [RUN]
2 of 5 OK created sql view model staging.stg_fear_greed ....... [OK in 1.12s]
3 of 5 START sql table model gold.dim_coins ................... [RUN]
3 of 5 OK created sql table model gold.dim_coins .............. [OK in 4.56s]
4 of 5 START sql table model gold.dim_dates ................... [RUN]
4 of 5 OK created sql table model gold.dim_dates .............. [OK in 3.21s]
5 of 5 START sql table model gold.fact_market_daily ........... [RUN]
5 of 5 OK created sql table model gold.fact_market_daily ...... [OK in 8.45s]

Finished running 2 view models, 3 table models in 22.34s.
Completed successfully

Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
```

Y para `dbt test`:

```
Running with dbt=1.8.x
Found 5 models, 7 tests, 2 sources

Concurrency: 1 threads (target='dev')

1 of 7 START test not_null_dim_coins_coin_id .................. [PASS in 1.23s]
2 of 7 START test unique_dim_coins_coin_id .................... [PASS in 1.45s]
...
7 of 7 START test assert_positive_prices ...................... [PASS in 2.34s]

Finished running 7 tests in 15.67s.
Completed successfully

Done. PASS=7 WARN=0 ERROR=0 SKIP=0 TOTAL=7
```

---

### 7.12 â€” Verificar los resultados

Comprueba que las tablas se crearon correctamente en Spark SQL:

```bash
# Abrir PySpark shell
make spark-shell
```

Dentro del shell:

```python
# Ver las tablas Gold creadas por dbt
spark.sql("SELECT * FROM cryptolake.gold.fact_market_daily LIMIT 5").show()
spark.sql("SELECT * FROM cryptolake.gold.dim_coins").show()
spark.sql("SELECT * FROM cryptolake.gold.dim_dates LIMIT 5").show()

# Contar registros
spark.sql("SELECT count(*) FROM cryptolake.gold.fact_market_daily").show()

# Ver las vistas staging
spark.sql("SELECT * FROM cryptolake.staging.stg_prices LIMIT 3").show()

# Consulta analÃ­tica de ejemplo: top coins por volatilidad
spark.sql("""
    SELECT coin_id, 
           ROUND(AVG(volatility_7d), 2) as avg_volatility,
           ROUND(AVG(price_usd), 2) as avg_price
    FROM cryptolake.gold.fact_market_daily
    GROUP BY coin_id
    ORDER BY avg_volatility DESC
""").show()

exit()
```

TambiÃ©n verifica en MinIO (http://localhost:9001) que las tablas Gold estÃ¡n en el bucket
`cryptolake-gold` gracias a la macro `create_table_as` que aÃ±adimos.

---

### 7.13 â€” Actualizar Makefile y hacer commit

AÃ±ade estas reglas al final de tu `Makefile`:

```makefile
# â”€â”€ dbt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
dbt-run: ## Ejecutar modelos dbt (staging â†’ gold)
	cd src/transformation/dbt_cryptolake && dbt run --profiles-dir .

dbt-test: ## Ejecutar tests dbt
	cd src/transformation/dbt_cryptolake && dbt test --profiles-dir .

dbt-all: ## Ejecutar dbt run + test
	$(MAKE) dbt-run
	$(MAKE) dbt-test
```

Actualiza tambiÃ©n la regla `pipeline` para que use dbt en lugar de `silver_to_gold.py`:

```makefile
pipeline: ## Ejecutar pipeline completo: Bronze â†’ Silver â†’ Gold (dbt)
	@echo "ğŸš€ Ejecutando pipeline completo..."
	$(MAKE) bronze-load
	$(MAKE) silver-transform
	$(MAKE) dbt-run
	$(MAKE) dbt-test
	@echo "âœ… Pipeline completado!"
```

Commit:

```bash
cd ~/Projects/cryptolake
git add .
git commit -m "feat: dbt Gold layer with star schema and data tests

- Spark Thrift Server added for JDBC connectivity
- dbt project with staging views + mart tables
- Star schema: dim_coins, dim_dates, fact_market_daily
- Window functions: MA7, MA30, volatility, price change %
- Custom macros: generate_schema_name, create_table_as (Iceberg LOCATION)
- Data quality tests: not_null, unique, assert_positive_prices
- Dual targets: dev (local) and prod (Docker/Airflow)"
```

---

## PARTE 8 (FASE 6): OrquestaciÃ³n con Apache Airflow

### 8.1 â€” Conceptos fundamentales

**Â¿QuÃ© es Airflow?**

Apache Airflow es un orquestador de workflows. Piensa en Ã©l como un "director de orquesta"
que coordina cuÃ¡ndo y en quÃ© orden se ejecuta cada pieza de tu pipeline de datos.

Sin Airflow, tendrÃ­as que ejecutar manualmente `make pipeline` cada dÃ­a. Con Airflow,
defines que el pipeline se ejecute automÃ¡ticamente a las 06:00 UTC, y si algo falla,
reintenta 2 veces, envÃ­a una alerta, y muestra exactamente dÃ³nde fallÃ³.

**Conceptos clave:**

- **DAG** (Directed Acyclic Graph): Un "grafo" que define tu workflow. "Dirigido" porque las
  flechas van en una direcciÃ³n (A â†’ B â†’ C). "AcÃ­clico" porque no hay ciclos (A â†’ B â†’ A no
  estÃ¡ permitido). En la prÃ¡ctica, es un archivo Python que define tareas y sus dependencias.

- **Task**: Una unidad de trabajo individual. Por ejemplo, "extraer datos de CoinGecko" o
  "ejecutar dbt run". Cada task es independiente y tiene su propio estado (success, failed,
  running, etc.).

- **TaskGroup**: Agrupa tareas relacionadas visualmente en la UI de Airflow. No cambia la
  ejecuciÃ³n, solo la organizaciÃ³n. Es como una carpeta para tareas.

- **Operators**: El "tipo" de tarea. Cada operator sabe ejecutar un tipo de trabajo especÃ­fico:
  - `BashOperator`: Ejecuta un comando bash
  - `PythonOperator`: Ejecuta una funciÃ³n Python
  - `SparkSubmitOperator`: EnvÃ­a un job a Spark (no lo usaremos por complejidad)

- **Schedule**: CuÃ¡ndo se ejecuta el DAG. Usa sintaxis cron:
  - `"0 6 * * *"` = cada dÃ­a a las 06:00
  - `"0 */4 * * *"` = cada 4 horas
  - `None` = solo ejecuciÃ³n manual (trigger)

**Nuestro DAG Master â€” CryptoLake Full Pipeline:**

```
TaskGroup: ingestion         TaskGroup: bronze       TaskGroup: silver
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ extract_coingecko    â”‚    â”‚                 â”‚    â”‚                  â”‚
â”‚         +            â”‚â”€â”€â”€â–¶â”‚ api_to_bronze   â”‚â”€â”€â”€â–¶â”‚ bronze_to_silver â”‚
â”‚ extract_fear_greed   â”‚    â”‚                 â”‚    â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                            â”‚
                    TaskGroup: quality      TaskGroup: gold  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                    â”‚                â”‚â—€â”€â”€â”€â”‚  dbt_run      â”‚â—€â”€â”˜
                    â”‚ quality_check  â”‚    â”‚     +         â”‚
                    â”‚                â”‚    â”‚  dbt_test     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 8.2 â€” Actualizar el Dockerfile de Airflow

Airflow necesita poder hacer dos cosas nuevas:

1. **Ejecutar spark-submit en el contenedor Spark** â†’ Necesita Docker CLI
2. **Ejecutar dbt** â†’ Necesita dbt-spark instalado

Reemplaza el archivo `docker/airflow/Dockerfile` completo:

```bash
cat > docker/airflow/Dockerfile << 'DOCKERFILE'
# ============================================================
# Apache Airflow para CryptoLake
# ============================================================
# Incluye:
# - Providers de Airflow para Spark
# - Docker CLI para ejecutar spark-submit en contenedores Spark
# - dbt-spark para la transformaciÃ³n Gold
# - Dependencias Python del proyecto
# ============================================================
FROM apache/airflow:2.9.3-python3.11

# â”€â”€ Instalar como root â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
USER root

# Docker CLI: para poder ejecutar "docker exec" en el contenedor de Spark.
# Esto es un patrÃ³n comÃºn en entornos de desarrollo local donde Airflow
# orquesta contenedores Docker hermanos (sibling containers).
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    docker.io \
    && rm -rf /var/lib/apt/lists/*

# AÃ±adir el usuario airflow al grupo docker para que pueda
# usar el Docker socket sin ser root
RUN groupadd -f docker && usermod -aG docker airflow

# â”€â”€ Instalar paquetes Python como airflow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
USER airflow

RUN pip install --no-cache-dir \
    apache-airflow-providers-apache-spark==4.7.1 \
    "dbt-spark[PyHive]==1.8.0" \
    requests==2.31.0 \
    pydantic==2.5.0 \
    pydantic-settings==2.1.0 \
    structlog==24.1.0
DOCKERFILE
```

---

### 8.3 â€” Actualizar docker-compose.yml para Airflow

Necesitamos dos cambios en los servicios de Airflow:

1. **Montar el Docker socket** â€” para que Airflow pueda ejecutar `docker exec` en el
   contenedor Spark. El Docker socket (`/var/run/docker.sock`) es el canal de comunicaciÃ³n
   con el Docker daemon.

2. **Montar el proyecto dbt** â€” ya tenemos `./src:/opt/airflow/src` montado, asÃ­ que
   el proyecto dbt ya es accesible en `/opt/airflow/src/transformation/dbt_cryptolake`.

Busca los servicios `airflow-webserver` y `airflow-scheduler` en tu `docker-compose.yml`
y aÃ±ade el Docker socket al bloque `volumes` de **ambos**:

```yaml
  # En airflow-webserver, secciÃ³n volumes:
  airflow-webserver:
    # ... (build, container_name, ports, environment igual que antes) ...
    volumes:
      - ./src/orchestration/dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - airflow-logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock   # â† AÃ‘ADIR
    # ... resto igual ...

  # En airflow-scheduler, secciÃ³n volumes:
  airflow-scheduler:
    # ... (build, container_name, environment igual que antes) ...
    volumes:
      - ./src/orchestration/dags:/opt/airflow/dags
      - ./src:/opt/airflow/src
      - airflow-logs:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock   # â† AÃ‘ADIR
    # ... resto igual ...
```

**Nota sobre permisos del Docker socket en macOS:**

En Docker Desktop para Mac, el socket se comparte automÃ¡ticamente y los permisos suelen
funcionar sin problemas. Si al ejecutar el DAG ves un error de permisos
(`Permission denied: /var/run/docker.sock`), la soluciÃ³n rÃ¡pida es aÃ±adir a ambos servicios
de Airflow:

```yaml
    # Solo si tienes problemas de permisos del socket:
    user: root
```

Esto hace que Airflow corra como root dentro del contenedor (seguro en desarrollo local,
no recomendado en producciÃ³n).

---

### 8.4 â€” Crear el DAG Master

Este es el corazÃ³n de la orquestaciÃ³n. Un solo archivo Python que define todo el pipeline.

```bash
cat > src/orchestration/dags/dag_full_pipeline.py << 'PYEOF'
"""
DAG Master de CryptoLake.

Ejecuta el pipeline completo de datos diariamente:
1. Ingesta batch (CoinGecko + Fear & Greed via APIs)
2. Bronze load (APIs â†’ Iceberg Bronze con Spark)
3. Silver processing (Bronze â†’ Silver con Spark)
4. Gold transformation (Silver â†’ Gold con dbt)
5. Data quality checks (dbt tests)

Schedule: Diario a las 06:00 UTC
Retry: 2 reintentos con 5 minutos entre cada uno
Timeout: 1 hora mÃ¡ximo por task

EjecuciÃ³n manual: TambiÃ©n se puede trigger desde la UI de Airflow.
"""
from datetime import datetime, timedelta

from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.utils.task_group import TaskGroup


# ================================================================
# ConfiguraciÃ³n por defecto para todas las tareas del DAG.
# Se puede sobreescribir en tareas individuales.
# ================================================================
default_args = {
    # Nombre del dueÃ±o (aparece en la UI de Airflow)
    "owner": "cryptolake",

    # depends_on_past=False: cada ejecuciÃ³n es independiente.
    # Si ayer fallÃ³, hoy se ejecuta igualmente.
    "depends_on_past": False,

    # No enviar emails al fallar (requerirÃ­a configurar SMTP)
    "email_on_failure": False,

    # Si una tarea falla, reintenta 2 veces
    "retries": 2,

    # Espera 5 minutos entre reintentos
    "retry_delay": timedelta(minutes=5),

    # Si una tarea tarda mÃ¡s de 1 hora, se cancela
    "execution_timeout": timedelta(hours=1),
}


# ================================================================
# DefiniciÃ³n del DAG
# ================================================================
# "with DAG(...) as dag:" es un context manager de Python.
# Todo lo que definamos dentro pertenece a este DAG.
# ================================================================
with DAG(
    # ID Ãºnico del DAG (aparece en la UI de Airflow)
    dag_id="cryptolake_full_pipeline",

    default_args=default_args,

    description="Pipeline completo: Ingesta â†’ Bronze â†’ Silver â†’ Gold â†’ Quality",

    # Schedule en formato cron: "minuto hora dÃ­a mes dÃ­a_semana"
    # "0 6 * * *" = a las 06:00, todos los dÃ­as, todos los meses
    schedule="0 6 * * *",

    # Fecha desde la que Airflow considerarÃ­a ejecutar este DAG.
    # Con catchup=False, NO ejecuta las fechas pasadas.
    start_date=datetime(2025, 1, 1),

    # catchup=False: No ejecutar retroactivamente para fechas pasadas.
    # Si activamos el DAG hoy, solo se ejecuta hoy, no intenta
    # ejecutar todos los dÃ­as desde start_date.
    catchup=False,

    # Tags para filtrar en la UI de Airflow
    tags=["cryptolake", "production"],

    # doc_md: la docstring de este archivo aparece como documentaciÃ³n
    # del DAG en la UI de Airflow
    doc_md=__doc__,

) as dag:

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GRUPO 1: INGESTA BATCH
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Descarga datos de las APIs externas.
    # CoinGecko y Fear & Greed se ejecutan en PARALELO (no hay
    # dependencia entre ellas â€” una no necesita a la otra).
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with TaskGroup("ingestion", tooltip="Descarga datos de APIs externas") as ingestion_group:

        extract_coingecko = BashOperator(
            task_id="extract_coingecko",
            # Ejecutamos el extractor Python directamente en el contenedor de Airflow.
            # El mÃ³dulo estÃ¡ montado en /opt/airflow/src/ via docker-compose volumes.
            bash_command=(
                "cd /opt/airflow && "
                "python -m src.ingestion.batch.coingecko_extractor"
            ),
        )

        extract_fear_greed = BashOperator(
            task_id="extract_fear_greed",
            bash_command=(
                "cd /opt/airflow && "
                "python -m src.ingestion.batch.fear_greed_extractor"
            ),
        )

        # No hay ">>" entre ellas = se ejecutan en paralelo

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GRUPO 2: BRONZE LOAD (APIs â†’ Iceberg Bronze)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Ejecuta spark-submit en el contenedor de Spark usando
    # "docker exec". Este patrÃ³n se llama "sibling containers":
    # Airflow usa el Docker socket para ejecutar comandos en
    # contenedores hermanos que comparten la misma red Docker.
    #
    # En producciÃ³n se usarÃ­a KubernetesPodOperator, EMROperator,
    # o Livy, pero para desarrollo local esto es lo mÃ¡s simple.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with TaskGroup("bronze_load", tooltip="Cargar datos en Iceberg Bronze") as bronze_group:

        api_to_bronze = BashOperator(
            task_id="api_to_bronze",
            bash_command=(
                "docker exec cryptolake-spark-master "
                "/opt/spark/bin/spark-submit "
                "/opt/spark/work/src/processing/batch/api_to_bronze.py"
            ),
        )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GRUPO 3: SILVER PROCESSING (Bronze â†’ Silver)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DeduplicaciÃ³n, limpieza y MERGE INTO. Todo con Spark.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with TaskGroup("silver_processing", tooltip="Limpiar y deduplicar en Silver") as silver_group:

        bronze_to_silver = BashOperator(
            task_id="bronze_to_silver",
            bash_command=(
                "docker exec cryptolake-spark-master "
                "/opt/spark/bin/spark-submit "
                "/opt/spark/work/src/processing/batch/bronze_to_silver.py"
            ),
        )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GRUPO 4: GOLD TRANSFORMATION (dbt)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # dbt se ejecuta directamente en el contenedor de Airflow
    # (dbt-spark estÃ¡ instalado en el Dockerfile de Airflow).
    # Conecta al Spark Thrift Server via JDBC.
    #
    # Usamos --target prod para que dbt use la configuraciÃ³n
    # de producciÃ³n (host: spark-thrift en vez de localhost).
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with TaskGroup("gold_transformation", tooltip="Modelado dimensional con dbt") as gold_group:

        dbt_run = BashOperator(
            task_id="dbt_run",
            bash_command=(
                "cd /opt/airflow/src/transformation/dbt_cryptolake && "
                "dbt run --profiles-dir . --target prod"
            ),
        )

        dbt_test = BashOperator(
            task_id="dbt_test",
            bash_command=(
                "cd /opt/airflow/src/transformation/dbt_cryptolake && "
                "dbt test --profiles-dir . --target prod"
            ),
        )

        # dbt_test se ejecuta DESPUÃ‰S de dbt_run
        dbt_run >> dbt_test

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GRUPO 5: DATA QUALITY
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Placeholder para Great Expectations (Fase 7).
    # Por ahora, los tests de dbt son nuestra validaciÃ³n de calidad.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with TaskGroup("data_quality", tooltip="ValidaciÃ³n de calidad de datos") as quality_group:

        quality_check = BashOperator(
            task_id="quality_summary",
            bash_command='echo "âœ… Data quality checks passed (dbt tests ran in gold_transformation group)"',
        )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # DEPENDENCIAS ENTRE GRUPOS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # El operador ">>" define el orden de ejecuciÃ³n:
    # ingestion â†’ bronze â†’ silver â†’ gold â†’ quality
    #
    # Esto se visualiza en la UI de Airflow como un grafo
    # de izquierda a derecha con flechas entre los grupos.
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    ingestion_group >> bronze_group >> silver_group >> gold_group >> quality_group
PYEOF
```

---

### 8.5 â€” Reconstruir y verificar

```bash
# Reconstruir imÃ¡genes con los cambios en el Dockerfile de Airflow
docker compose down
docker compose up -d --build

# Espera ~90 segundos para que todos los servicios arranquen
sleep 90

# Verificar que todos estÃ¡n running
make status
```

Verifica que tienes estos servicios nuevos o actualizados:
- `cryptolake-spark-thrift` â†’ running (puerto 10000)
- `cryptolake-airflow-webserver` â†’ running (puerto 8083)
- `cryptolake-airflow-scheduler` â†’ running

---

### 8.6 â€” Activar y ejecutar el DAG en Airflow

1. Abre **http://localhost:8083** en tu navegador
2. Login con `admin` / `admin`
3. VerÃ¡s el DAG `cryptolake_full_pipeline` en la lista (con tag "production")
4. El DAG estÃ¡ **pausado** por defecto (el toggle a la izquierda estÃ¡ en OFF)
5. **Antes de activarlo**, haz una ejecuciÃ³n manual de prueba:
   - Haz clic en el nombre del DAG para entrar en su vista detalle
   - Haz clic en el botÃ³n **"Trigger DAG"** (icono â–¶ï¸ arriba a la derecha)
   - Confirma la ejecuciÃ³n

6. VerÃ¡s el DAG ejecutÃ¡ndose en la pestaÃ±a **"Graph"**:
   - Los nodos se ponen **verde oscuro** cuando estÃ¡n en ejecuciÃ³n
   - Se ponen **verde claro** cuando completan exitosamente
   - Se ponen **rojo** si fallan

7. Para ver los logs de una tarea especÃ­fica:
   - Haz clic en el nodo de la tarea (ej: `bronze_load.api_to_bronze`)
   - Selecciona **"Log"** en el popup

8. Una vez que la ejecuciÃ³n manual funcione correctamente, activa el toggle
   para que se ejecute automÃ¡ticamente segÃºn el schedule (diario a las 06:00 UTC).

**Troubleshooting comÃºn:**

Si `api_to_bronze` o `bronze_to_silver` fallan con error de Docker, verifica:

```bash
# Â¿Puede Airflow acceder al Docker socket?
docker exec cryptolake-airflow-scheduler docker ps

# Si falla con "permission denied", aÃ±ade "user: root" al servicio
# airflow-scheduler en docker-compose.yml y reconstruye
```

Si `dbt_run` falla con error de conexiÃ³n, verifica:

```bash
# Â¿EstÃ¡ el Thrift Server corriendo?
docker logs cryptolake-spark-thrift 2>&1 | tail -10

# Â¿Puede Airflow llegar al Thrift Server?
docker exec cryptolake-airflow-scheduler \
    python -c "from pyhive import hive; conn = hive.connect('spark-thrift', 10000); print('OK')"
```

---

### 8.7 â€” Actualizar Makefile y hacer commit

AÃ±ade estas reglas al final de tu `Makefile`:

```makefile
# â”€â”€ Airflow â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
airflow-trigger: ## Trigger manual del DAG completo en Airflow
	docker exec cryptolake-airflow-scheduler \
	    airflow dags trigger cryptolake_full_pipeline

airflow-status: ## Ver estado de la Ãºltima ejecuciÃ³n del DAG
	docker exec cryptolake-airflow-scheduler \
	    airflow dags list-runs -d cryptolake_full_pipeline --limit 5
```

Commit:

```bash
cd ~/Projects/cryptolake
git add .
git commit -m "feat: Airflow orchestration with full pipeline DAG

- DAG master with 5 TaskGroups: ingestion â†’ bronze â†’ silver â†’ gold â†’ quality
- Spark jobs via docker exec (sibling container pattern)
- dbt Gold layer triggered from Airflow with --target prod
- Airflow Dockerfile updated with Docker CLI + dbt-spark
- Docker socket mounted for container orchestration
- Schedule: daily at 06:00 UTC with 2 retries"
```

---

## Resumen de lo implementado

### Fase 5 â€” dbt Gold Layer
- Spark Thrift Server como punto de entrada JDBC
- Proyecto dbt con staging (views) + marts (tables)
- Star schema: `dim_coins`, `dim_dates`, `fact_market_daily`
- Window functions: MA7, MA30, volatilidad, cambio porcentual
- Macros personalizadas: `generate_schema_name`, `create_table_as` (LOCATION para Iceberg)
- Tests automÃ¡ticos de calidad de datos

### Fase 6 â€” Airflow Orchestration
- DAG master `cryptolake_full_pipeline` con 5 TaskGroups
- EjecuciÃ³n diaria automÃ¡tica a las 06:00 UTC
- Spark jobs orquestados via Docker exec (patrÃ³n sibling containers)
- dbt ejecutado directamente desde Airflow con target prod
- Reintentos automÃ¡ticos y timeouts configurados

### Pipeline completo ahora:

```
[APIs] â†’ [Ingesta Python] â†’ [Spark: Bronze] â†’ [Spark: Silver] â†’ [dbt: Gold] â†’ [Tests]
   â†‘          â†‘                    â†‘                 â†‘               â†‘            â†‘
 CoinGecko  Airflow Task      spark-submit      spark-submit     dbt run      dbt test
 Fear&Greed  Group 1           Group 2           Group 3         Group 4      Group 4
```
